{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "## Lesson Objectives\n",
    "- Understand the basics of concept learning and hypothesis spaces in ML.\n",
    "- Learn the FIND-S algorithm: its purpose, steps, and limitations.\n",
    "- Implement FIND-S in Python using Jupyter Notebook, with data loaded from a CSV file.\n",
    "- Evaluate the hypothesis generated and discuss its practical applications.\n",
    "- Gain hands-on experience through the integrated experiment.\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python programming (lists, loops, file handling).\n",
    "- Familiarity with Jupyter Notebook (since you're using it—great choice for interactive ML experimentation!).\n",
    "- Installed libraries: Pandas (for CSV reading) and NumPy (optional for data manipulation). Install via `!pip install pandas numpy` in your Jupyter cell if needed.\n",
    "- No prior ML knowledge required; we'll build from basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "### Concept Learning and Hypothesis Spaces\n",
    "\n",
    "#### What is Concept Learning?\n",
    "Concept learning is a subfield of machine learning where the goal is to identify a \"concept\" (a rule or pattern) from examples. Imagine teaching a child to recognize \"birds\": you show positive examples (eagle, sparrow) and negative ones (bat, airplane). The child learns a hypothesis like \"has wings and feathers.\"\n",
    "\n",
    "In Machine Learning (ML) terms:\n",
    "\n",
    "- **Instances** $(X)$: The set of input data samples. Each instance $x \\in X$ is described by a set of attributes (features).\n",
    "- **Target Concept** $(c)$: The unknown function we are trying to learn. For binary classification:\n",
    "  $$\n",
    "  c: X \\rightarrow \\{0, 1\\}\n",
    "  $$\n",
    "  where $c(x) = 1$ indicates a *positive* instance and $c(x) = 0$ a *negative* instance.\n",
    "- **Hypothesis** $(h)$: A proposed function that approximates the target concept:\n",
    "  $$\n",
    "  h: X \\rightarrow \\{0, 1\\}\n",
    "  $$\n",
    "  where $h \\approx c$, meaning the hypothesis tries to closely match the true labels given by $c$.\n",
    "- **Hypothesis Space** $(H)$: The set of all possible hypotheses that can be considered, based on the representation language or model class:\n",
    "  $$\n",
    "  H = \\{ h \\mid h: X \\rightarrow \\{0,1\\} \\}\n",
    "  $$\n",
    "- **Training Examples**: A set of labeled data points:\n",
    "  $$\n",
    "  D = \\{ \\langle x_1, c(x_1) \\rangle, \\langle x_2, c(x_2) \\rangle, \\dots, \\langle x_n, c(x_n) \\rangle \\}\n",
    "  $$\n",
    "  where each $x_i \\in X$ is an instance, and $c(x_i) \\in \\{0,1\\}$ is its label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The FIND-S Algorithm\n",
    "FIND-S (Find Specific) is a simple, greedy algorithm for concept learning. It finds the **most specific hypothesis** that fits all positive training examples. It ignores negative examples, assuming the hypothesis space is conjunctive (AND-based rules) and there's no noise in data.\n",
    "\n",
    "**Key Points**:\n",
    "- **Concept Learning**: A learning problem where the system learns to identify or classify an object or pattern based on examples.\n",
    "- **Most Specific Hypothesis**: The most specific hypothesis is the one that matches only the training examples provided, i.e., it doesn’t generalize too much.\n",
    "\n",
    "**Key Assumptions**: \n",
    "- The target concept is in the hypothesis space (representable as conjunctions of attribute values).\n",
    "- Data is noise-free and consistent.\n",
    "- Attributes are discrete (categorical).\n",
    "\n",
    "**Hypothesis Representation**:\n",
    "A hypothesis $h$ is a conjunction of constraints on attributes. Each attribute $A_i$ can take a specific value $v_i$, a wildcard `?` (meaning \"any value\"), or `∅` (meaning \"no value\" or \"empty\").\n",
    "$$\n",
    "h = \\langle A_1, A_2, \\ldots, A_n \\rangle\n",
    "$$\n",
    "where $A_i \\in \\{v_i, ?, \\emptyset\\}$.\n",
    "\n",
    "- `?` → any value (**generalization**)\n",
    "- `∅` or specific value → **constraint**\n",
    "\n",
    "**Example (Attributes: `[Color, Shape]`)**:\n",
    "- `[\"Red\", \"Circle\"]` → Red **AND** Circle\n",
    "- `[\"?\", \"Circle\"]` → Any color, but **must be** Circle\n",
    "- `[\"Red\", \"?\"]` → Must be Red, any shape\n",
    "- `[\"?\", \"?\"]` → Any color and any shape (most general)\n",
    "- `[\"∅\", \"Circle\"]` → Matches nothing (inconsistent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIND-S Algorithm Steps\n",
    "\n",
    "1. **Initialize** the hypothesis $h$ to the most specific possible. This can be done in two ways:\n",
    "   - Set $h$ to a vector of empty constraints:\n",
    "     $$\n",
    "     h = \\langle \\emptyset, \\emptyset, \\ldots, \\emptyset \\rangle\n",
    "     $$\n",
    "   - Alternatively, set $h$ to the first positive training example $x_1$ (excluding its target label):\n",
    "     $$\n",
    "     h = \\langle x_{1,1}, x_{1,2}, \\ldots, x_{1,n} \\rangle\n",
    "     $$\n",
    "\n",
    "2. **For each** positive training example $x = \\langle x_1, x_2, \\ldots, x_n \\rangle$:\n",
    "   - Compare $x$ with the current hypothesis $h = \\langle h_1, h_2, \\ldots, h_n \\rangle$.\n",
    "   - For each attribute $i$ from $1$ to $n$:\n",
    "     - If $h_i = \\emptyset$ (uninitialized), set $h_i := x_i$.\n",
    "     - Else if $h_i \\neq x_i$ (mismatch), set $h_i := ?$ (generalize).\n",
    "     - Else ($h_i = x_i$, match), keep $h_i$ unchanged.\n",
    "\n",
    "   This update rule can be summarized as:\n",
    "   $$\n",
    "   h_i \\leftarrow\n",
    "   \\begin{cases}\n",
    "       x_i & \\text{if } h_i = \\emptyset \\\\\n",
    "       ? & \\text{if } h_i \\neq x_i \\text{ and } h_i \\neq \\emptyset \\\\\n",
    "       h_i & \\text{if } h_i = x_i\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "3. **Ignore negative examples.** FIND-S does not consider them; it only focuses on generalizing over positive instances.\n",
    "\n",
    "4. **Output** the final hypothesis $h$, which is the most specific one that covers all positive examples.\n",
    "\n",
    "### Example\n",
    "Given three attributes, the initial hypothesis might be:\n",
    "$$\n",
    "h = \\langle \\emptyset, \\emptyset, \\emptyset \\rangle\n",
    "$$\n",
    "After processing positive examples, it could evolve to something like:\n",
    "$$\n",
    "h = \\langle \\text{Sunny}, ?, \\text{High} \\rangle\n",
    "$$\n",
    "\n",
    "## Limitations of FIND-S Algorithm\n",
    "- **Cannot handle noise or inconsistent data**: FIND-S assumes all positive examples are correct and consistent, so it fails if data contains errors.\n",
    "- **Finds only the most specific hypothesis**: It does not explore or find all possible hypotheses that fit the data, limiting generalization.\n",
    "- **Incomplete without other algorithms**: For completeness, FIND-S is often paired with algorithms like **LIST-THEN-ELIMINATE** or the **Candidate Elimination Algorithm (CEA)**, which maintain a **version space** — the set of all hypotheses consistent with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing FIND-S in Jupyter Notebook\n",
    "\n",
    "### Step 1: Prepare Your Jupyter Notebook Environment\n",
    "- Open Jupyter Notebook (via Anaconda or command line: `jupyter notebook`).\n",
    "- Create a new notebook named \"FIND-S_Experiment.ipynb\".\n",
    "- In the first cell, import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement the FIND-S Algorithm\n",
    "#### Cell 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "        Sky  Temp Humidity    Wind Water Forecast WaterSport\n",
      "0    Cloudy  Cold     High  Strong  Warm   Change        Yes\n",
      "1     Sunny  Warm   Normal  Strong  Warm     Same        Yes\n",
      "2     Sunny  Warm     High  Strong  Warm     Same        Yes\n",
      "3    Cloudy  Cold     High  Strong  Warm   Change         No\n",
      "4     Sunny  Warm     High  Strong  Cool   Change        Yes\n",
      "5      Rain  Mild     High    Weak  Cool   Change         No\n",
      "6      Rain  Cool   Normal    Weak  Cool     Same         No\n",
      "7  Overcast  Cool   Normal  Strong  Warm     Same        Yes\n",
      "\n",
      "Positive Examples:\n",
      "['Cloudy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'Yes']\n",
      "['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'Yes']\n",
      "['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'Yes']\n",
      "['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'Yes']\n",
      "['Overcast', 'Cool', 'Normal', 'Strong', 'Warm', 'Same', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "data = pd.read_csv('finds.csv')\n",
    "\n",
    "# Display the data for verification\n",
    "print(\"Training Data:\")\n",
    "print(data)\n",
    "\n",
    "# Extract attributes (exclude target) and target\n",
    "attributes = data.columns[:-1]  # All columns except the last\n",
    "target = data.columns[-1]\n",
    "\n",
    "# Convert to list of lists for easier processing\n",
    "instances = data.values.tolist()\n",
    "\n",
    "# Separate positive examples (only process these)\n",
    "positive_examples = [inst for inst in instances if inst[-1] == 'Yes']\n",
    "\n",
    "print(\"\\nPositive Examples:\")\n",
    "for ex in positive_examples:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Explanation**:\n",
    "> - Columns: Attributes + Target (EnjoySport: Yes/No).\n",
    "> - Positive examples: Rows where EnjoySport = \"Yes\".\n",
    "> - Negative: \"No\" (ignored in FIND-S).\n",
    "> - We use Pandas to read the CSV effortlessly.\n",
    "> - `positive_examples` filters only \"Yes\" rows, as FIND-S ignores negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: Implement FIND-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Hypothesis: ['Cloudy', 'Cold', 'High', 'Strong', 'Warm', 'Change']\n",
      "Updated Hypothesis after example ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'Yes'] : ['?', '?', '?', 'Strong', 'Warm', '?']\n",
      "Updated Hypothesis after example ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'Yes'] : ['?', '?', '?', 'Strong', 'Warm', '?']\n",
      "Updated Hypothesis after example ['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'Yes'] : ['?', '?', '?', 'Strong', '?', '?']\n",
      "Updated Hypothesis after example ['Overcast', 'Cool', 'Normal', 'Strong', 'Warm', 'Same', 'Yes'] : ['?', '?', '?', 'Strong', '?', '?']\n",
      "\n",
      "Most Specific Hypothesis:\n",
      "Sky: ?\n",
      "Temp: ?\n",
      "Humidity: ?\n",
      "Wind: Strong\n",
      "Water: ?\n",
      "Forecast: ?\n"
     ]
    }
   ],
   "source": [
    "# Initialize the most specific hypothesis\n",
    "# Assume attributes are categorical; start with first positive example\n",
    "if not positive_examples:\n",
    "    print(\"No positive examples found!\")\n",
    "else:\n",
    "    # Start with the first positive example (exclude target)\n",
    "    hypothesis = positive_examples[0][:-1]\n",
    "    print(\"\\nInitial Hypothesis:\", hypothesis)\n",
    "\n",
    "    # Process each subsequent positive example\n",
    "    for example in positive_examples[1:]:\n",
    "        example_attrs = example[:-1]  # Exclude target\n",
    "        for i in range(len(hypothesis)):\n",
    "            if hypothesis[i] != example_attrs[i]:\n",
    "                hypothesis[i] = '?'  # Generalize to any value\n",
    "        print(\"Updated Hypothesis after example\", example, \":\", hypothesis)\n",
    "\n",
    "    # Final hypothesis\n",
    "    print(\"\\nMost Specific Hypothesis:\")\n",
    "    for attr, val in zip(attributes, hypothesis):\n",
    "        print(f\"{attr}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation** (Step-by-Step):\n",
    "- **Initialization**: Set `hypothesis` to the attributes of the first positive example (e.g., ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same']).\n",
    "- **Iteration**: For each next positive example:\n",
    "  - Compare attribute-by-attribute.\n",
    "  - If mismatch, replace with \"?\" (e.g., if Humidity differs, set to \"?\").\n",
    "- **Output**: The final hypothesis, e.g., ['Sunny', 'Warm', '?', 'Strong', '?', '?'] meaning \"Sunny AND Warm AND Strong AND (any Humidity, Water, Forecast)\".\n",
    "- This matches the algorithm: It generalizes only as needed to cover all positives, staying as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the ML Model (Hypothesis)\n",
    "FIND-S isn't a predictive model like classifiers, but we can \"evaluate\" it by:\n",
    "- **Consistency Check**: Apply the hypothesis to training data. For a new instance, if it matches the specific parts (ignoring ?), predict positive.\n",
    "- **Generalization Test**: Create test data (e.g., add to CSV) and see if it correctly classifies.\n",
    "- Metrics: Since it's binary, compute accuracy on hold-out data. But note: FIND-S overfits to positives; real evaluation needs negatives (use CEA for boundaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions on Training Data:\n",
      "Instance: ['Cloudy', 'Cold', 'High', 'Strong', 'Warm', 'Change'], Predicted: Yes, Actual: Yes\n",
      "Instance: ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same'], Predicted: Yes, Actual: Yes\n",
      "Instance: ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same'], Predicted: Yes, Actual: Yes\n",
      "Instance: ['Cloudy', 'Cold', 'High', 'Strong', 'Warm', 'Change'], Predicted: Yes, Actual: No\n",
      "Instance: ['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change'], Predicted: Yes, Actual: Yes\n",
      "Instance: ['Rain', 'Mild', 'High', 'Weak', 'Cool', 'Change'], Predicted: No, Actual: No\n",
      "Instance: ['Rain', 'Cool', 'Normal', 'Weak', 'Cool', 'Same'], Predicted: No, Actual: No\n",
      "Instance: ['Overcast', 'Cool', 'Normal', 'Strong', 'Warm', 'Same'], Predicted: Yes, Actual: Yes\n"
     ]
    }
   ],
   "source": [
    "# Simple evaluation: Check consistency with all data\n",
    "def predict(hypothesis, instance):\n",
    "    for h_val, i_val in zip(hypothesis, instance):\n",
    "        if h_val != '?' and h_val != i_val:\n",
    "            return 'No'\n",
    "    return 'Yes'\n",
    "\n",
    "print(\"\\nPredictions on Training Data:\")\n",
    "for inst in instances:\n",
    "    pred = predict(hypothesis, inst[:-1])\n",
    "    actual = inst[-1]\n",
    "    print(f\"Instance: {inst[:-1]}, Predicted: {pred}, Actual: {actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications and Real-World Examples\n",
    "- **Rule-Based Systems**: In e-commerce, FIND-S-like logic learns user preferences from positive reviews (e.g., \"Likes: Red AND Small\" for products).\n",
    "- **Anomaly Detection**: Focus on \"normal\" (positive) behaviors to define specific rules, ignoring outliers.\n",
    "- **Bioinformatics**: Learn specific gene patterns from positive disease samples.\n",
    "- **Extension**: Modern ML uses FIND-S ideas in inductive logic programming (e.g., in Prolog-based systems) or as a baseline for explainable AI.\n",
    "- **Case Study**: In agriculture (relevant to India), use weather data to learn \"Crop Success\" hypothesis: \"Sunny AND Warm AND High Humidity\" from successful harvests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
