{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d326eea1-a460-4685-b24b-e8b8b2e90b0f",
   "metadata": {},
   "source": [
    "---\n",
    "**Created on** *Thu Oct 03 06:29:02 2025*  \n",
    "**@author**: **Chaitanya.M, DVR and Dr. HS Mic College, Kanchikacharla**\n",
    "\n",
    "[**GitHub Repository**](https://github.com/chaitanyamokkapati/MIC-ECE-MACHINE-LEARNING-LAB/tree/main/MACHINE%20LEARNING/LAB%20ACTIVITIES/LAB%20ACTIVITY%207)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6be1b-8430-4672-9c27-e78105ef55b4",
   "metadata": {},
   "source": [
    "# Lab Activity 7: **Build an Artificial Neural Network by implementing the Back propagation algorithm and test the same using appropriate data sets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e12f19-fcfd-4a69-b42a-c62e3355098b",
   "metadata": {},
   "source": [
    "---\n",
    "## Objective\n",
    "\n",
    "> The objective of this notebook is to **understand and implement** the *Backpropagation Algorithm* in an Artificial Neural Network (ANN) and test its performance using an appropriate dataset.\n",
    "\n",
    "1. **Understand Neural Networks**\n",
    "   - Learn the basic structure and functioning of neural networks.\n",
    "\n",
    "2. **Implement Backpropagation**\n",
    "   - Gain hands-on experience in implementing the *backpropagation algorithm* for training a neural network.\n",
    "\n",
    "3. **Data Preprocessing**\n",
    "   - Learn how to **normalize** input and output data for better performance.\n",
    "\n",
    "4. **Model Training**\n",
    "   - Train the model and observe how **weights and biases** are adjusted to minimize error.\n",
    "\n",
    "5. **Evaluate Performance**\n",
    "   - Test the trained model and evaluate its accuracy using metrics like *Mean Squared Error (MSE)*.\n",
    "\n",
    "6. **Experiment with Hyperparameters**\n",
    "   - Explore how changes in **hyperparameters** affect the model's performance.\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6568f-80db-475f-9ca0-12c7da046dfc",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa66722-c223-453a-ab35-314ca07836d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55559977-b38b-400d-be29-1a70d170b750",
   "metadata": {},
   "source": [
    "### 2. Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2df12a-4f5d-496a-bf2f-9136c89a24e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Input Data (X):\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      "Normalized Output Data (y):\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the Data\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)  # Input: [sleep, study hours]\n",
    "y = np.array(([92], [86], [89]), dtype=float)  # Output: Percentage scores\n",
    "\n",
    "# Normalize the data (feature scaling)\n",
    "X = X / np.amax(X, axis=0)  # Normalize the input data to the range 0-1\n",
    "y = y / 100  # Normalize the output (percentage) between 0 and 1\n",
    "\n",
    "# Print the normalized data\n",
    "print(\"Normalized Input Data (X):\")\n",
    "print(X)\n",
    "print(\"\\nNormalized Output Data (y):\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000a9af-a72f-4d3b-8a97-caee0b768f86",
   "metadata": {},
   "source": [
    "### 3. Define Sigmoid Function and Its Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48598dea-396b-447d-b315-0b7530cb05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Sigmoid Activation Function and its Derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9117c78-c762-4e5f-b407-90dbc7dac4f5",
   "metadata": {},
   "source": [
    "### 4. Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a0dada-6477-4dc8-bd35-4d2bee0400ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights and Biases:\n",
      "Hidden Layer Weights (wh): [[0.35989388 0.71659598 0.64170643]\n",
      " [0.47133242 0.65676361 0.06833671]]\n",
      "Hidden Layer Biases (bh): [[0.35378989 0.63880291 0.0408965 ]]\n",
      "Output Layer Weights (wout): [[0.93121904]\n",
      " [0.91535343]\n",
      " [0.53574644]]\n",
      "Output Layer Biases (bout): [[0.31753841]]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize Variables\n",
    "epoch = 7000  # Number of training iterations (epochs)\n",
    "lr = 0.1  # Learning rate\n",
    "inputlayer_neurons = 2  # Number of features in the dataset (sleep and study hours)\n",
    "hiddenlayer_neurons = 3  # Number of neurons in the hidden layer\n",
    "output_neurons = 1  # Output neuron (percentage)\n",
    "\n",
    "# Initialize weights and biases\n",
    "wh = np.random.uniform(size=(inputlayer_neurons, hiddenlayer_neurons))  # Weights for the hidden layer\n",
    "bh = np.random.uniform(size=(1, hiddenlayer_neurons))  # Bias for the hidden layer\n",
    "wout = np.random.uniform(size=(hiddenlayer_neurons, output_neurons))  # Weights for the output layer\n",
    "bout = np.random.uniform(size=(1, output_neurons))  # Bias for the output layer\n",
    "\n",
    "# Print the initial weights and biases\n",
    "print(\"Initial Weights and Biases:\")\n",
    "print(\"Hidden Layer Weights (wh):\", wh)\n",
    "print(\"Hidden Layer Biases (bh):\", bh)\n",
    "print(\"Output Layer Weights (wout):\", wout)\n",
    "print(\"Output Layer Biases (bout):\", bout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4cf0b-8496-48f8-9029-a5b2b1a48f31",
   "metadata": {},
   "source": [
    "### 5. Training the Network (Forward and Backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f0877a-fcdd-46c4-aeba-33e1bd3e5d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, MSE: 0.00033466670528857533\n",
      "Epoch 1000, MSE: 0.0003292027858906191\n",
      "Epoch 2000, MSE: 0.0003244026468271401\n",
      "Epoch 3000, MSE: 0.00031964544045554635\n",
      "Epoch 4000, MSE: 0.00031492954306642563\n",
      "Epoch 5000, MSE: 0.0003102535651317531\n",
      "Epoch 6000, MSE: 0.0003056163292936562\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the Neural Network\n",
    "for i in range(epoch):\n",
    "    # Forward Propagation\n",
    "    hinp1 = np.dot(X, wh)  # Linear combination of input and weights (hidden layer input)\n",
    "    hinp = hinp1 + bh  # Adding bias to hidden layer input\n",
    "    hlayer_act = sigmoid(hinp)  # Apply sigmoid activation function to hidden layer\n",
    "    \n",
    "    outinp1 = np.dot(hlayer_act, wout)  # Linear combination of hidden layer output and output weights\n",
    "    outinp = outinp1 + bout  # Adding bias to output layer input\n",
    "    output = sigmoid(outinp)  # Apply sigmoid activation function to output layer\n",
    "    \n",
    "    # Backpropagation (Updating Weights and Biases)\n",
    "    EO = y - output  # Error at the output layer\n",
    "    outgrad = derivatives_sigmoid(output)  # Gradient of the output layer\n",
    "    d_output = EO * outgrad  # Error gradient of output layer\n",
    "    \n",
    "    EH = d_output.dot(wout.T)  # Error at the hidden layer\n",
    "    hiddengrad = derivatives_sigmoid(hlayer_act)  # Gradient of the hidden layer\n",
    "    d_hiddenlayer = EH * hiddengrad  # Error gradient of the hidden layer\n",
    "    \n",
    "    # Update the weights and biases\n",
    "    wout += hlayer_act.T.dot(d_output) * lr  # Update output layer weights\n",
    "    bout += np.sum(d_output, axis=0, keepdims=True) * lr  # Update output layer bias\n",
    "    wh += X.T.dot(d_hiddenlayer) * lr  # Update hidden layer weights\n",
    "    bh += np.sum(d_hiddenlayer, axis=0, keepdims=True) * lr  # Update hidden layer bias\n",
    "\n",
    "    if i % 1000 == 0:  # Print error every 1000 epochs\n",
    "        mse = np.mean((y - output) ** 2)  # Calculate Mean Squared Error (MSE)\n",
    "        print(f\"Epoch {i}, MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b866f-b24b-423c-9ba8-a924ae8757ff",
   "metadata": {},
   "source": [
    "### 6. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10707b34-f964-4e30-952e-5dbd165d3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Complete\n",
      "\n",
      "Predicted Output for Test Data:\n",
      "[[90.25621979]\n",
      " [88.90420361]]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Test the Model (Evaluate Performance)\n",
    "print(\"\\nTraining Complete\\n\")\n",
    "\n",
    "# Test Data (New input for prediction)\n",
    "test_input = np.array(([4, 8], [2, 7]), dtype=float)  # Example test data (new sleep and study hours)\n",
    "test_input = test_input / np.amax(test_input, axis=0)  # Normalize the test data\n",
    "\n",
    "# Perform forward propagation with the trained weights\n",
    "hinp_test = np.dot(test_input, wh)  # Input to hidden layer\n",
    "hinp_test = hinp_test + bh  # Add bias to hidden layer input\n",
    "hlayer_act_test = sigmoid(hinp_test)  # Apply sigmoid activation to hidden layer\n",
    "\n",
    "outinp_test = np.dot(hlayer_act_test, wout)  # Input to output layer\n",
    "outinp_test = outinp_test + bout  # Add bias to output layer input\n",
    "output_test = sigmoid(outinp_test)  # Apply sigmoid activation to output layer\n",
    "\n",
    "print(\"Predicted Output for Test Data:\")\n",
    "print(output_test * 100)  # Convert back to percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bff4b-5e7f-484d-a21a-07434d53d09c",
   "metadata": {},
   "source": [
    "### 7. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c003720-cc8e-4e0d-80f9-f831cc6be389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Squared Error on training data: 0.0003010214310793555\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Performance Evaluation (Mean Squared Error)\n",
    "mse = np.mean((y - output) ** 2)  # Calculate Mean Squared Error (MSE)\n",
    "print(\"\\nMean Squared Error on training data:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
